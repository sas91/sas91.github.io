[{"authors":["Aswin Shanmugam Subramanian","Chao Weng","Shinji Watanabe","Meng Yu","Shi-Xiong Zhang","Yong Xu","Dong Yu"],"categories":null,"content":"","date":1603324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603324800,"objectID":"5eead71a8399e0ff1533a6941af4761e","permalink":"https://sas91.github.io/publication/dasr/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/publication/dasr/","section":"publication","summary":"","tags":null,"title":"Directional ASR: A New Paradigm for E2E Multi-Speaker Speech Recognition with Source Localization","type":"publication"},{"authors":["Aswin Shanmugam Subramanian","Chao Weng","Meng Yu","Shi-Xiong Zhang","Yong Xu","Shinji Watanabe","Dong Yu"],"categories":null,"content":"","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579824000,"objectID":"89cc7a0f4f1cfa9c56c93a883ecb3a11","permalink":"https://sas91.github.io/publication/e2e-lgasr/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/publication/e2e-lgasr/","section":"publication","summary":"","tags":null,"title":"Far-Field Location Guided Target Speech Extraction using End-to-End Speech Recognition Objectives","type":"publication"},{"authors":["Yuya Fujita","Aswin Shanmugam Subramanian","Motoi Omachi","Shinji Watanabe"],"categories":null,"content":"","date":1577318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577318400,"objectID":"8dbd8ca9e6a2b163b33a20ecad37685b","permalink":"https://sas91.github.io/publication/lt_d_conv-yj/","publishdate":"2019-12-26T00:00:00Z","relpermalink":"/publication/lt_d_conv-yj/","section":"publication","summary":"","tags":null,"title":"Attention-based ASR with Lightweight and Dynamic Convolutions","type":"publication"},{"authors":["Toru Taniguchi","Aswin Shanmugam Subramanian","Xiaofei Wang","Dung Tran","Yuya Fujita","Shinji Watanabe"],"categories":null,"content":"","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"f1314e1bc86e6972d2fce9100436d4b3","permalink":"https://sas91.github.io/publication/yj-waspaa19/","publishdate":"2019-10-20T00:00:00Z","relpermalink":"/publication/yj-waspaa19/","section":"publication","summary":"","tags":null,"title":"Generalized Weighted-Prediction-Error Dereverberation with Varying Source Priors for Reverberant Speech Recognition","type":"publication"},{"authors":["Aswin Shanmugam Subramanian","Xiaofei Wang","Murali Karthick Baskar","Shinji Watanabe","Toru Taniguchi","Dung Tran","Yuya Fujita"],"categories":null,"content":"","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"57c13d9fc091e40173c5ca72b9f97102","permalink":"https://sas91.github.io/publication/e2e-waspaa19/","publishdate":"2019-10-20T00:00:00Z","relpermalink":"/publication/e2e-waspaa19/","section":"publication","summary":"","tags":null,"title":"Speech Enhancement Using End-to-End Speech Recognition Objectives","type":"publication"},{"authors":["Naoyuki Kanda","Rintaro Ikeshita","Shota Horiguchi","Yusuke Fujita","Kenji Nagamatsu","Xiaofei Wang","Vimal Manohar","Nelson Enrique Yalta Soplin","Matthew Maciejewski","Szu-Jui Chen","Aswin Shanmugam Subramanian","Ruizhi Li","Zhiqi Wang","Jason Naradowsky","Paola Garcia-Perera","Gregory Sell"],"categories":null,"content":"","date":1536278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536278400,"objectID":"c661e865223202bdcf38ad508bd27654","permalink":"https://sas91.github.io/publication/chime5-hitachi-jhu/","publishdate":"2018-09-07T00:00:00Z","relpermalink":"/publication/chime5-hitachi-jhu/","section":"publication","summary":"","tags":null,"title":"The Hitachi/JHU CHiME-5 system: Advances in speech recognition for everyday home environments using multiple microphone arrays","type":"publication"},{"authors":["Szu-Jui Chen","Aswin Shanmugam Subramanian","Hainan Xu","Shinji Watanabe"],"categories":null,"content":"","date":1535846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535846400,"objectID":"35aad5fde86f715e3e14c1e3390f1227","permalink":"https://sas91.github.io/publication/chime4-adv/","publishdate":"2018-09-02T00:00:00Z","relpermalink":"/publication/chime4-adv/","section":"publication","summary":"","tags":null,"title":"Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline","type":"publication"},{"authors":["Aswin Shanmugam Subramanian","Szu-Jui Chen","Shinji Watanabe"],"categories":null,"content":"","date":1535846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535846400,"objectID":"a8ef5b66e1f1e8b902fc5c0724ab5463","permalink":"https://sas91.github.io/publication/st-learning-is18/","publishdate":"2018-09-02T00:00:00Z","relpermalink":"/publication/st-learning-is18/","section":"publication","summary":"","tags":null,"title":"Student-Teacher Learning for BLSTM Mask-based Speech Enhancement","type":"publication"},{"authors":["Aswin Shanmugam S"],"categories":null,"content":"","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"287ee4d918a6a944e84f010569a11923","permalink":"https://sas91.github.io/publication/ms-thesis/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/publication/ms-thesis/","section":"publication","summary":"","tags":null,"title":"A Hybrid Approach to Segmentation of Speech Using Signal Processing Cues and Hidden Markov Models","type":"publication"},{"authors":["S Aswin Shanmugam","Hema Murthy"],"categories":null,"content":"","date":1410652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410652800,"objectID":"88d9fae59a4e5183056d7316ba40aa96","permalink":"https://sas91.github.io/publication/hybrid-segmentation/","publishdate":"2014-09-14T00:00:00Z","relpermalink":"/publication/hybrid-segmentation/","section":"publication","summary":"","tags":null,"title":"A Hybrid Approach to Segmentation of Speech Using Group Delay Processing and HMM Based Embedded Reestimation","type":"publication"},{"authors":["Abhijit Pradhan","Aswin Shanmugam S","Anusha Prakash","Kamakoti Veezhinathan","Hema Murthy"],"categories":null,"content":"","date":1378684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1378684800,"objectID":"b481ff779f5fcbb93eb44a88e3356c50","permalink":"https://sas91.github.io/publication/syllable-hts/","publishdate":"2013-09-09T00:00:00Z","relpermalink":"/publication/syllable-hts/","section":"publication","summary":"A statistical parametric speech synthesis system uses triphones, phones or full context phones to address the problem of co-articulation. In this paper, syllables are used as the basic units in the parametric synthesiser. Conventionally full context phones in a Hidden Markov Model (HMM) based speech synthesis framework are modeled with a fixed number of states. This is because each phoneme corresponds to a single indivisible sound. On the other hand a syllable is made up of a sequence of one or more sounds. To accommodate this variation, a variable number of states are used to model a syllable. Although a variable number of states are required to model syllables, a syllable captures co-articulation well since it is the smallest production unit. A syllable based speech synthesis system therefore does not require a well designed question set. The total number of syllables in a language is quite high and all of them cannot be modeled. To address this issue, a fallback unit is modeled instead. The quality of the proposed system is comparable to that of the phoneme based system in terms of DMOS and WER.","tags":null,"title":"A Syllable Based Statistical Text to Speech System","type":"publication"}]